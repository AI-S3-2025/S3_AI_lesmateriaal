{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wiskundige technieken les 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook bij les 4 van de leerlijn wiskundige technieken van S3 - AI. \n",
    "\n",
    "© Auteur: Rianne van Os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In de vorige lessen heb je alle kennis opgedaan die nodig is om het gradient descent algoritme te implementeren voor het vinden van een beste fit bij een linaire regressie. In deze gaan we toewerken naar een functie die gradient descent gebruikt om een lijn te fitten op een dataset met 1 feature- en 1 targetvariabele. \n",
    "\n",
    "**Voorbereiding les 4:**\n",
    "- Neem de theorie en opdrachten uit de eerste 3 lessen nog eens door om je geheugen op te frissen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import benodigde libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Root mean squared error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Als eerste voorbeeld gaan we kijken naar een datasetje met maar één feature $x$ en een target $y$. We zijn dan op zoek naar de beste lijn door de punten $x$ en $y$. Hierbij is de 'beste' lijn, de lijn die zorgt voor de kleinste *root mean squared error*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We bekijken datasetje dat we ook in de les over lineaire regressie hebben gebruikt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "personen = pd.DataFrame({'lengte': [1.678, 1.755, 1.764, 1.821, 1.809, 1.851, 1.848, 1.902], \n",
    "                         'gewicht': [62.2, 67.2, 72.7, 76.4, 75.2, 81.3, 87.4, 90.4]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nu gaan we op zoek naar een waarde $a$ en een waarde $b$ zodat we een zo goed mogelijke lijn door deze punten kunnen maken. We willen dus een $a$ en $b$ vinden, zodat bij voorkeur voor iedere $i$ geldt: \n",
    "$$gewicht_i = a \\cdot lengte_i + b$$\n",
    "\n",
    "Deze $a$ wordt ook wel de *weight* en $b$ de bias van je model genoemd. Heb je meerdere feature-variabelen, bijvoorbeeld niet alleen `lengte` maar ook `leeftijd`, dan zou je op zoek gaan naar *weights* $a_1$ en $a_2$ en een *bias* $b$, zodat $$gewicht_i = a_1 \\cdot lengte_i + a_2 \\cdot leeftijd_i + b.$$\n",
    "\n",
    "Dit geval met meer features zul je later zelf moeten bekijken, we gaan in deze les uit van slechts één feature-variabele."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Als we de data plotten zien we al dat we nooit een rechte lijn zullen vinden die precies door alle punten gaat. voor $a$, de richtingscoefficent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "personen.plot.scatter('lengte','gewicht')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Om een gevoel te krijgen voor de waarde van de *weight* $a$ en *bias* $b$ gaan we eerst handmatig de formule voor een lijn opstellen door enkele punten opstellen. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Opdracht 4.1\n",
    "a. Kies 2 observaties uit de personen dataset en stel een formule op voor de lijn die door die punten gaat.\n",
    "\n",
    "b. Bereken de RMSE van de hele dataset, als je uitgaat van de lijn die je in a. hebt opgesteld. Dit nemen we als baseline voor je eigen implementatie van *gradient descent*. \n",
    "\n",
    "c. Wat is het gewicht van een persoon van 1.60m en van een persoon van 1.80m volgens de lijn die je hebt opgesteld?\n",
    "\n",
    "\n",
    "Ter opfrissing: de *root mean squared error* is de wortel van de *mean squared error*, en die is gegeven door:\n",
    "\n",
    "$$ MSE = \\frac{1}{n} \\sum_{i = 1}^{n} (y_i - \\hat{y}_i)^2$$ \n",
    "waarbij $\\hat{y}_i$ de voorspelde waarde is. In dit 2-dimensionale geval is $\\hat{y}_i = a\\cdot x_i + b$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minimum van de RMSE vinden met gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nu gaan we *gradient descent* implementeren om te kijken of we een betere fit kunnen vinden voor de lijn door deze dataset. Hiervoor gaan we op zoek naar het minimum van de functie van de root mean squared error. Eerder hebben je al gezien dat de root mean squared error gewoon de wortel van de mean squared error is. Als een de wortel van een functie een minimum heeft, heeft de functie zelf dat ook. Daarom is het voldoende om het minimum van de *mean squared error* te vinden, dat scheelt weer een wortel-teken die de berekeningen bemoeilijkt. De *mean squared error* ziet er zo uit:\n",
    "\n",
    "\n",
    "$$ MSE = \\frac{1}{n} \\sum_{i = 1}^{n} (y_i - \\hat{y}_i)^2$$\n",
    "waarbij $\\hat{y}_i = a \\cdot x_i +b$, oftewel de door de regressielijn voorspelde waarde. We kunnen dit dus ook schrijven als:\n",
    "$$ MSE = \\frac{1}{n} \\sum_{i = 1}^{n} (y_i - (a \\cdot x_i +b))^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En er geldt, hoe kleiner deze error, hoe beter de lijn *fit* op de data. We willen dus op zoek naar waarden $a$ en $b$ die ervoor zorgen dat de functie $$f(a,b) = \\frac{1}{n} \\sum_{i = 1}^{n} (y_i - (a \\cdot x_i +b))^2$$ zo klein mogelijk wordt. \n",
    "**Merk op** \n",
    "- Dit is een functie met variabelen $a$ en $b$. De $x_i$ en $y_i$ zijn bekend, dit zijn namelijk de features (lengte en gewicht) uit de dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Om hier gradient descent op toe te passen, hebben we de afgeleiden $\\frac{df}{da}$ en $\\frac{df}{db}$ nodig. Als je direct weet hoe je die moet opstellen, mag je dat nu doen. Als je geen idee hebt, maak dan de volgende opdracht om daar stap voor stap te komen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Opdracht 4.2\n",
    "a. We starten met het opstellen van de functie voor de MSE als we 2 datapunten hebben. Stel we hebben de punten (1.82, 76) en (1.61, 71), dan is de bijbehorende MSE te berekenen met:\n",
    "$$ f(a,b) = \\frac{1}{2} \\left( (76 - (a \\cdot 1.82 + b))^2 + (71 - (a \\cdot 1.61 + b))^2 \\right)$$\n",
    "Bepaal van deze functie de partieel afgeleiden $\\frac{df}{da}$ en $\\frac{df}{db}$. \n",
    "\n",
    "b. We gaan nu het algemene geval met 2 datapunten bekijken, stel we hebben de datapunten $(x_1, y_1)$ en $(x_2, y_2)$, wat is dan de functie voor de MSE? En wat zijn de partieel afgeleiden?\n",
    "\n",
    "c. Met 3 datapunten krijg je de volgende MSE:\n",
    "$$f (a,b) = \\frac{1}{3} \\left( (y_1 - (a \\cdot x_1 + b))^2 + (y_2 - (a \\cdot x_2 + b))^2 + (y_3 - (a \\cdot x_3 + b))^2 \\right)$$ \n",
    "wat we met een sommatie-teken kunnen schrijven als:\n",
    "$$f (a,b) = \\frac{1}{3} \\sum_{i=1}^{3} (y_i - (a \\cdot x_i + b))^2$$\n",
    "Bepaal de partieel afgeleiden $\\frac{df}{da}$ en $\\frac{df}{db}$ van deze functie. Doe dat door eerst de functie zonder sommatie-teken te differentieren en daarna te kijken hoe je de partieel afgeleiden daarna weer met sommatie-teken kunt schrijven.\n",
    "\n",
    "d. Probeer nu de partieel afgeleiden van de algemene functie $$f (a,b) = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - (a \\cdot x_i + b))^2$$ te vinden. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Opdracht 4.3\n",
    "\n",
    "a. Implementeer *gradient descent* voor $f(a,b)$. Neem `x = personen[lengte]` en `y=personen[gewicht]` zoals hierboven. Maak gebruik van de functionaliteit van numpy om de afgeleiden te implementeren.  \n",
    "\n",
    "\n",
    "b. Wat is de beste fit die je kunt vinden? Welke RMSE hoort daarbij? Welke learning rate lijkt goed te werken? Maakt het nog uit welke beginwaarde je kiest voor de *weight* en *bias*?\n",
    "\n",
    "c. Gebruik de gevonden weight en bias (oftewel $a$ en $b$) om te bepalen wat (volgens dit model) het verwachte gewicht is van een persoon van lengte 1.80m en van een persoon met een lengte van 1.60m."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mogelijke verbeteringen\n",
    "Hieronder benoemen we een aantal manieren om de implementatie van *gradient descent* te verbeteren. De theorie hierover kun je zelf doornemen aan de hand van de bronnen en dan proberen toe te passen in je eigen gradient descent implementatie. Voor het portfolio-item ben je verplicht 1 van deze technieken te implementeren (al hoeft het niet perse tot een verbetering van je algoritme te leiden).\n",
    "\n",
    "### Batch gradient descent\n",
    "In de implementatie van *gradient descent* die je hebt gemaakt, bereken je de afgeleiden van de *loss* functie voor alle datapunten tegelijk. Dit wordt ook wel *batch gradient descent* genoemd. Je kunt in plaats daarvan ook in iedere stap de afgeleiden berekenen voor een deel van de datapunten, dit wordt *mini-batch gradient descent* genoemd. Doe je dit in iedere stap voor 1 datapunt, dan heet het *stochastic gradient descent*.\n",
    "Hier wordt dit uitgelegd: https://medium.com/data-science/batch-mini-batch-stochastic-gradient-descent-7a62ecba642a\n",
    "\n",
    "### Normaliseren van de data\n",
    "Het kan zijn dat je algoritme sneller convergeert (dus sneller het minimum vindt) als je de data vantevoren normaliseert. Hier wordt uitgelegd waarom dat zo is: https://www.youtube.com/watch?v=to-UeYMPm34\n",
    "Of lees het antwoord deze op vraag hier: https://www.quora.com/Why-is-it-important-to-scale-your-inputs-in-gradient-descent\n",
    "\n",
    "### Learning rate decay\n",
    "Het kan zijn dat je algoritme sneller convergeert (dus sneller een minimum vindt) als je de *learning rate* verlaagt naarmate je dichter bij het minimum komt. Dit heet *learning rate decay*. Hier wordt dit uitgelegd: https://www.geeksforgeeks.org/machine-learning/learning-rate-decay/\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "S3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
