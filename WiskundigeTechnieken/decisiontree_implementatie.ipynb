{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0bfd7f20",
   "metadata": {},
   "source": [
    "# Deep dive decision trees\n",
    "\n",
    "Notebook bij de les over de werking van decision trees\n",
    "© Auteur: Rianne van Os (ism gemini 2.5)\n",
    "\n",
    "\n",
    "**Voorbereiding**\n",
    "Het is belangrijk dat je al eens recursief een boomstructuur opgebouwd hebt met een Tree en een Node klasse. Dit kun je oefenen in het notebook recursie_les1.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201e0371",
   "metadata": {},
   "source": [
    "In de machine learning lessen heb je geleerd wat een Decision Tree is en hoe je die moet trainen met behulp van sklearn. In deze les gaan we dit algoritme beter bekijken en een deel ervan zelf implementeren.\n",
    "\n",
    "*Note*: zet je copilot uit en gebruik geen generatieve AI voor deze opdrachten. Je gaat aan de hand van kleine stappen een eigen beslisboom maken, uiteraard kan iedere LLM deze stappen zo voor je uitwerken. Daar leer je niks van en je ontneemt jezelf ook het plezier van het in staat zijn een werkend algoritme te implementeren.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7337ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importeer benodigde libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336a3e8a",
   "metadata": {},
   "source": [
    "## Decision trees - korte herhaling\n",
    "We hebben al eerder een decision tree getrained en deze gevisualiseerd. Hieronder zie je een stukje van een boom die we in de eerste les over machine learning getraind hebben:\n",
    "\n",
    "![dt](./../afbeeldingen/WiskundigeTechnieken/decisiontree.png \"Stuk van decision tree\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5c23a6",
   "metadata": {},
   "source": [
    "We zien hier per node van de boom een aantal zaken:\n",
    "- Op welke feature er gesplitst wordt en op welke splittingswaarde (threshold) dat gebeurt\n",
    "- De gini impurity index van de Node. Dit geeft aan hoe zuiver of onzuiver de node is.\n",
    "- Hoeveel samples er in totaal in de node zitten.\n",
    "- Hoeveel samples er van iedere soort in die node zitten (dit is de parameter value)\n",
    "- De soort die het meest voorkomt in die node. Dit zou de voorspelling zijn.\n",
    "\n",
    "De laatste nodes, die we *leaf nodes* noemen, zijn hier helemaal zuiver: er komen alleen nog pinguins van 1 soort voor (en deze boom is dus waarschijnlijk overfit).\n",
    "\n",
    "Om zo'n boom te maken moet voor iedere nieuwe node bepaald worden op welke feature gesplitst moet worden en op welke threshold dat moet gebeuren. Vervolgens kan dan zo'n boom recursief opgenbouwd worden. Hoe dit precies gebeurt ga je in de volgende opdrachten zien en uiteindelijk ga je dit allemaal zelf implementeren.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e2d492",
   "metadata": {},
   "source": [
    "## Het bepalen van de split\n",
    "Er zijn twee maten waarmee bepaald kan worden wat voor een gegeven node de beste volgende split gaat zijn. Dit kan op basis van de *gini impurity index* of op basis van *entropie*. In de boom hierboven zag je dat er gebruik gemaakt werd van de gini index, maar wij gaan kijken naar entropie. \n",
    "\n",
    "Om uit te leggen wat entropie is, gebruiken we onderstaand datasetje. Daarin staan gegevens over studenten die wel of niet een vak gehaald hebben. We willen kijken of je op basis van de vooropleiding en het aantal uur dat de student geleerd heeft kunt voorspellen of het vak gehaald wordt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6bd3cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultaten_df = pd.DataFrame({'vooropleiding': ['MBO', 'MBO', 'anders', 'havo', 'anders', 'MBO', 'havo'],\n",
    "                     'uur': [23, 18, 32, 28, 11, 15, 2], \n",
    "                     'gehaald': [True, True, True, False, False, False, False]})\n",
    "resultaten_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92cfd47d",
   "metadata": {},
   "source": [
    "We kunnen de eerste split maken op de kolom vooropleiding of op de kolom uur. Wat zou logisch zijn? En welke splittingswaarde moeten we gebruiken? Stel we kiezen vooropleiding als kolom om eerst op te splitten, gaan we dan voor `vooropleiding == MBO` en `vooropleiding != MBO`, of zou `havo` een betere eerste keuze zijn? En kiezen we voor de kolom uur, gaan we dan voor `uur <16.5`? Of zou een andere waarde beter zijn?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b5c752",
   "metadata": {},
   "source": [
    "### Opdracht 1\n",
    "Bedenk zelf op welke feature je als eerst gaat splitten en wat de bijbehorende splittingswaarde is. Leg uit waarom dat een logische keuze is."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d012e80d",
   "metadata": {},
   "source": [
    "## Entropie\n",
    "\n",
    "Waarschijnlijk heb je bij bovenstaande opdracht gekeken hoe je een split kunt kiezen die ervoor zorgt dat te `true` en `false` waarden zo goed mogelijk gescheiden worden in 2 nodes. We zeggen hier dat je kiest voor een split die de nodes zo *zuiver* mogelijk maken. Waarbij een *zuivere* node enkel datapunten heeft uit 1 enkele klasse, en een *onzuivere* node heeft datapunten uit verschillende klassen. Nu helpt het om die 'zuiverheid' uit te kunnen berekenen. De maat die daarvoor gebruikt kan worden is *entropie*. (Een andere maat is gini-impurity, die mag je zelf uitzoeken voor boven niveau)\n",
    "\n",
    "Entropie kun je als volgt berekenen: voor een set data `S` met verschillende klassen, wordt de Entropie `H(S)` berekend als:\n",
    "\n",
    "$ H(S) = - \\sum_{i=1}^{c} p_i \\log_2(p_i) $\n",
    "\n",
    "Hierbij geldt:\n",
    "*   $ c $ is het aantal unieke klassen.\n",
    "*   $ p_i $ is het aandeel van samples in set `S` die behoren tot klasse $ i $. Dus bijvoorbeeld 2/7 als 2 van de 7 datapunten bij klasse $i$ horen.\n",
    "\n",
    "Merk verder op dat we het logaritme nemen met grondtal 2. In numpy is dit `np.log2(x)`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90dd986b",
   "metadata": {},
   "source": [
    "Bestaat de set S van labels maar uit twee klassen (bijvoorbeeld True en False), dan kunnen de we formule makkelijk zonder sommatie-teken schrijven en dan krijg je:\n",
    "    $$ H(S) = - p \\log_2(p) - (1-p) \\log_2(1-p) $$\n",
    "Waarbij $p$ het aandeel True-labels is (dus opnieuw bijvoorbeeld 2/7 als 2 van de 7 labels True is)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1786fe3a",
   "metadata": {},
   "source": [
    "### Opdracht 2\n",
    "Beantwoord de volgende vragen door de berekeningen handmatig (met pen en papier) uit te voeren:\n",
    "\n",
    "1. Bereken de entropie van bovenstaande datasetje, als we uitgaan van de kolom `gehaald`.\n",
    "2. Je kunt deze berekening voor entropie ook gebruiken als je meer dan 2 klasses hebt. Stel dat we een decision tree hebben getraind om op basis van gegeven features te voorspellen of een datapunt bij klasse A, B of C hoort. Een bepaalde node in de decision tree bestaat uit 10 samples, waarvan 5 uit klasse A, 3 uit klasse B en 2 uit klasse C. Wat is de entropie van deze node?\n",
    "3. Wat is de entropie van een node die bestaat uit 3 datapunten van klasse A en 3 uit klasse B?\n",
    "4. Wat is de entropie van een node die bestaat uit 4 datapunten van klasse B?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07aefaf0",
   "metadata": {
    "tags": [
     "docent"
    ]
   },
   "outputs": [],
   "source": [
    "#uitwerking 1\n",
    "-3/7*np.log2(3/7) - 4/7 * np.log2(4/7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859a1eb4",
   "metadata": {
    "tags": [
     "docent"
    ]
   },
   "outputs": [],
   "source": [
    "#uitwerking 2\n",
    "-5/10*np.log2(5/10) - 3/10 * np.log2(3/10)-2/10*np.log2(2/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dbcf51b",
   "metadata": {
    "tags": [
     "docent"
    ]
   },
   "outputs": [],
   "source": [
    "#uitwerking 3\n",
    "-3/6*np.log2(3/6) - 3/6 * np.log2(3/6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01fb91fd",
   "metadata": {
    "tags": [
     "docent"
    ]
   },
   "outputs": [],
   "source": [
    "#uitwerking 4\n",
    "-1*np.log2(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909bdb24",
   "metadata": {},
   "source": [
    "### Entropie bij 2 klassen\n",
    "Als er twee klassen zijn (dus $c = 2$), dan ligt de waarde van de entropie tussen de 0 (alles behoort tot één klasse) en 1 (meetwaarden zijn precies verdeeld over de twee klassen).\n",
    "\n",
    "NB. Als je het aandeel $p_1$ van de eerste klasse $c_1$ weet, dan weet je ook het aandeel $p_2 = 1 - p_1$ van de tweede klasse $c_2$. Bij méér dan twee klassen gaat dit natuurlijk niet meer op. \n",
    "\n",
    "Bij 2 klassen geeft de volgende grafiek de verandering in entropie weer ten opzichte van het aandeel van klasse $c_1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a8c43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, dpi=100)\n",
    "\n",
    "x = np.linspace(.001,.999,100)\n",
    "y = -x * np.log2(x) + -(1-x) * np.log2(1-x)\n",
    "ax.set_xlabel(r\"Aandeel van klasse $c_1$\")\n",
    "ax.set_ylabel(r\"Entropie $E(S)$\")\n",
    "\n",
    "plt.plot(x,y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805dcbdc",
   "metadata": {},
   "source": [
    "## Opdracht 3\n",
    "1. Schrijf een functie `calculate_entropy` die de entropie van een numpy array met labels kan berekenen. Deze moet ook werken voor een classificatieprobleem met meer dan 2 klassen. Controleer hiermee je antwoorden uit de vorige opdracht.\n",
    "2. Bereken met deze functie de entropie van de hele dataset (dus van de kolom `gehaald`). Bereken daarna de entropie van de nodes die je krijgt als je resultaten_df splitst op `vooropleiding == MBO`. Doe hetzelfde voor de split `uur < 16.5`. Kun je op basis hiervan al bedenken wat een betere split zou zijn?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9628c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_entropy(labels = np.array) -> float:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2901b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Entropie van hele kolom {calculate_entropy(resultaten_df['gehaald'])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d322e5",
   "metadata": {
    "tags": [
     "docent"
    ]
   },
   "outputs": [],
   "source": [
    "print(f\"Entropie van vooropleiding = MBO: {calculate_entropy(resultaten_df.loc[resultaten_df['vooropleiding']=='MBO','gehaald'])}\")\n",
    "print(f\"Entropie van vooropleiding != MBO: {calculate_entropy(resultaten_df.loc[resultaten_df['vooropleiding']!='MBO','gehaald'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34165eb",
   "metadata": {
    "tags": [
     "docent"
    ]
   },
   "outputs": [],
   "source": [
    "print(f\"Entropie van uur <=15: {calculate_entropy(resultaten_df.loc[resultaten_df['uur']<=16.5,'gehaald'])}\")\n",
    "print(f\"Entropie van uur > 15 {calculate_entropy(resultaten_df.loc[resultaten_df['uur']>16.5,'gehaald'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac29828",
   "metadata": {},
   "source": [
    "## Information gain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3b64fd",
   "metadata": {},
   "source": [
    "Met de entropie kunnen we uitrekenen hoe zuiver 1 node in de boom is, maar daarmee hebben we nog geen goede split bepaald. Daarvoor hebebn we een nieuwe term nodig, namelijk de *information gain*. Om de information gain uit te rekenen, bepaal je het verschil tussen de entropie van de *parent node* en de *gewogen gemiddelde* entropie van de *child nodes*. We willen namelijk dat de nieuwe nodes (*child nodes*) zuiverder zijn dan de oorspronkelijke node (*parent node*). Een hogere Information Gain betekent dan een betere split. Het is de split die de meeste \"informatie\" oplevert of de \"onzekerheid\" het meest reduceert. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3d8fbd",
   "metadata": {},
   "source": [
    "De formule voor de information gain is:\n",
    "\n",
    "$ IG(P, Children) = H(P) - ( \\frac{|C_1|}{|P|} H(C_1) + \\frac{|C_2|}{|P|} H(C_2) )$\n",
    "\n",
    "*   $ H(P) $ is de entropie van de *parent node*.\n",
    "*   $ |C_i| $ is het aantal samples in *child node* $ C_i $.\n",
    "*   $ |P| $ is het aantal samples in de *parent node*.\n",
    "*   $ H(C_i) $ is de entropie van kind-set $ C_i $.\n",
    "*   $ \\frac{|C_i|}{|P|} $ is het gewicht van de *childe node*, gebaseerd op het aantal samples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38c1ed4",
   "metadata": {},
   "source": [
    "### Opdracht 4\n",
    "Bereken de infomation gain voor de split van `resultaten_df` op `vooropleiding == MBO` en die op `uur < 16.5`. Welke heeft de hoogste *information gain* en is dus de beste split?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb02960b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "09b61a8b",
   "metadata": {},
   "source": [
    "### Opdracht 5\n",
    "Implementeer een functie `calculate_information_gain`. De eerste parameter is een numpy array met labels uit de *parent node* en de tweede parameter is een lijst met 2 numpy arrays met de labels van de 2 *child nodes*. Maak bij de implementatie gebruik van de functie `calculate_entropy`. Controleer hiermee de gevonden waarde uit de vorige opdracht."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7bd749",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_information_gain(parent_labels : np.array, child_labels : list) -> float:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24f8be4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b4c4f9ac",
   "metadata": {},
   "source": [
    "## Data splitten"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724ee1ce",
   "metadata": {},
   "source": [
    "Nu we weten hoe je de kwaliteit van een split kunt bepalen, kunnen we toe gaan werken naar een functie die, gegeven een dataset, zelf de beste split kan bepalen. Voordat we die functie implementeren, starten we eerst met een functie split_data, die gegeven de features, de target, de feature waarop gesplit moet worden (bijvoorbeeld `vooropleiding`) en de splittingswaarde (bijvoorbeeld `mbo`), de labels van de child-nodes teruggeeft. Deze functie kun je vervolgens gebruiken om daadwerkelijk de beste split te gaan bepalen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f015222b",
   "metadata": {},
   "source": [
    "**Merk op:** Het splitten van een categorische variabele werkt net iets anders dan bij numerieke variabele. Deze functie heeft daarom ook nog een paramater `is_numeric` nodig om te bepalen hoe gesplit moet worden."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a427f712",
   "metadata": {},
   "source": [
    "### Opdracht 6\n",
    "Implementeer de functie `split_data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aca6f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(features: pd.DataFrame, target: np.array, feature: str, threshold: any, is_numeric: bool = False) -> tuple:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304f28bb",
   "metadata": {
    "tags": [
     "docent"
    ]
   },
   "outputs": [],
   "source": [
    "def split_data(features: pd.DataFrame, target: np.array, feature: str, threshold: any, is_numeric: bool = False) -> list:\n",
    "    \"\"\"\n",
    "    Split the data based on a feature and a threshold.\n",
    "    \n",
    "    Args:\n",
    "        features (pd.DataFrame): DataFrame with features\n",
    "        target (pd.Series): Series with target labels\n",
    "        feature (str): Feature to split on\n",
    "        threshold (str): Threshold value for the split\n",
    "        is_numeric (bool): Whether the feature is numeric or categorical\n",
    "        \n",
    "    Returns:\n",
    "        list: List with two arrays containing the labels of the child nodes\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "#test\n",
    "split_data(resultaten_df, resultaten_df['gehaald'], 'uur', 15, is_numeric=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9799b1",
   "metadata": {},
   "source": [
    "### Opdracht 7\n",
    "Nu hebben we alle functies die nodig zijn om, gegeven een dataset, de beste split te bepalen. Maak gebruik van de eerder geimplementeerde functies om de functie `find_best_split` te implementeren. Deze functie krijgt de features en de target mee en returned de naam van de feature waarop gesplitst moet worden en het splitcriteruim (de threshold). We gaan er in deze implementatie vanuit dat er bij categorische waarden op iedere waarde gesplit kan worden. \n",
    "\n",
    "Om de beste split op een numerieke waarde te bepalen, bepaal je eerst alle gemiddelden van twee opeenvolgende waarden, en dat zijn dan je mogelijke splittingswaarden. Bijvoorbeeld, als de waarden in een kolom 1, 3, 4 en 7 zijn, dan zijn de mogelijke splittingswaarden 2 (gemiddelde van 1 en 3), 3.5 (gemiddelde van 3 en 4) en 5.5 (gemiddelde van 4 en 7). Code om dit efficient te doen zullen we je geven:\n",
    "```python\n",
    "    unique_values = np.unique(features[feature])\n",
    "    unique_values.sort()\n",
    "    thresholds = (unique_values[:-1] + unique_values[1:]) / 2\n",
    "```\n",
    "Bij een categorische feature zijn de splittingswaarden gewoon alle unieke categorien.\n",
    "\n",
    "Roep de functie aan om de beste split van `resultaten_df` te bepalen. Lees vervolgens een dataset in waarop we eerder een classificatie hebben gedaan (bijvoorbeeld `pinguins.csv`) en test je functie daarop. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7aedd6b",
   "metadata": {
    "tags": [
     "docent"
    ]
   },
   "outputs": [],
   "source": [
    "def find_best_split(features: pd.DataFrame, target: pd.Series) -> tuple:\n",
    "    \"\"\"\n",
    "    Find the best split for the given features and target.\n",
    "    \n",
    "    Args:\n",
    "        features (pd.DataFrame): DataFrame with features\n",
    "        target (pd.Series): Series with target labels\n",
    "        \n",
    "    Returns:\n",
    "        tuple: Feature name and threshold value for the best split\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "#test\n",
    "find_best_split(resultaten_df.drop(columns='gehaald'), resultaten_df['gehaald'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07ea312",
   "metadata": {
    "tags": [
     "docent"
    ]
   },
   "outputs": [],
   "source": [
    "#test met pinguins dataset\n",
    "pinguins_df = pd.read_csv('../databronnen/pinguins.csv')\n",
    "pinguins_df.dropna(inplace=True)\n",
    "find_best_split(pinguins_df.drop(columns='species'), pinguins_df['species'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67824b12",
   "metadata": {},
   "source": [
    "## Een decision tree implementeren - portfolio item\n",
    "\n",
    "Nu zijn we zo ver dat je deze logica samen kunt gaan voegen in een DecisionTree klasse. We gaan deze beslisboom weer recursief opbouwen, net zoals je in de recursie les gedaan hebt. Ook hier ga je een Node klasse maken, die zelf bijhoudt welke labels er in die node zitten, op welke feature en op welke threshold gesplitst moet worden en wat zijn 'child nodes' zijn. \n",
    "\n",
    "Daarnaast implementeer je een DecisionTree class. Hierin komt alle logica bij elkaar. Deze heeft weer 2 recursieve elementen. \n",
    "- Het bouwen van de Tree, dat gedaan wordt met `Tree.fit()` waarin `Tree._build_recursive()` aangeroepen wordt.\n",
    "- Het doen van een voorspelling met `Tree.predict()`. Deze kan een dataframe met meerdere rijen meekrijgen, en roept vervolgens per rij `Tree._predict_one()`, die op zijn beurt weer recursief opgebouwd wordt door `Tree._predict_one_recursive()` aan te roepen.\n",
    "\n",
    "De base case is telkens het geval waarbij de Node een leaf is (dus waarbij Node.is_leaf = True), in alle andere gevallen moet iedere node de recursieve functie op de left en de right child aanroepen. \n",
    "\n",
    "Het is erg handig om de boom te kunnen printen, de code om dat recursief te doen krijg je van ons. Wel moet je `Node.__repr__()` zelf schrijven, zodat de nodes geprint kunnen worden.\n",
    "\n",
    "Hieronder staan de klassediagrammen die tot een goede implementatie kunnen leiden. Je mag hiervan afwijken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4988898",
   "metadata": {},
   "outputs": [],
   "source": [
    "#uitwerking voor het printen van de DT, rest van de functies moeten zelf geschreven worden.\n",
    "class DecisionTree():\n",
    "    \n",
    "    #TODO: implementeer andere functies zelf\n",
    "    \n",
    "    def __repr__(self):\n",
    "        # Return the full tree representation as a string so print(tree) works\n",
    "        if self.root is None:\n",
    "            return \"<empty tree>\"\n",
    "        return self._repr_recursive(self.root, prefix = \"\")\n",
    "                \n",
    "    def _repr_recursive(self, node: Node, prefix=\"\", type = None) -> str:\n",
    "        \"\"\"Return a string representation of the subtree rooted at `node`.\n",
    "\n",
    "        This function builds lines recursively and returns a single string\n",
    "        (with embedded newlines). \n",
    "        \"\"\"\n",
    "        if node is None:\n",
    "            return \"\"\n",
    "\n",
    "        # determine line prefix and new prefix for children\n",
    "        if type == 'left':\n",
    "            line_prefix = prefix + \"├── L: \"\n",
    "            new_prefix = prefix + \"│   \"\n",
    "        elif type == 'right':\n",
    "            line_prefix = prefix + \"└── R: \"\n",
    "            new_prefix = prefix + \"    \"\n",
    "        else:  # root\n",
    "            line_prefix = \"\"\n",
    "            new_prefix = prefix\n",
    "\n",
    "        lines = [line_prefix + str(node)]\n",
    "\n",
    "        # recursively collect left and right subtree strings\n",
    "        left_str = self._repr_recursive(node.left_child, new_prefix, type='left')\n",
    "        right_str = self._repr_recursive(node.right_child, new_prefix, type='right')\n",
    "\n",
    "        if left_str:\n",
    "            lines.append(left_str)\n",
    "        if right_str:\n",
    "            lines.append(right_str)\n",
    "\n",
    "        return \"\\n\".join(lines)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee9908e",
   "metadata": {},
   "source": [
    "Het klassediagram. Hiermee kun je het werkend krijgen, maar je mag ook voor andere functies kiezen. Merk op: je hebt heel veel al een keer geimplementeerd in de voorgaande opdrachten."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28293922",
   "metadata": {},
   "source": [
    "```mermaid\n",
    "classDiagram\n",
    "direction TB\n",
    "\n",
    "class Node {\n",
    "    +pd.Series target\n",
    "    +str split_feature\n",
    "    +any split_threshold\n",
    "    +Node left_child\n",
    "    +Node right_child\n",
    "    +bool is_leaf\n",
    "    +_get_majority_class() (unique, counts, majority)\n",
    "    +__repr__() str\n",
    "}\n",
    "\n",
    "class DecisionTree {\n",
    "    +Node root\n",
    "    +int max_depth\n",
    "    +__repr__() str\n",
    "    +_calculate_entropy(np.array) float\n",
    "    +_calculate_information_gain(np.array, pd.Series, pd.Series) float\n",
    "    +_split_data(pd.DataFrame, np.array, str, any, bool) tuple\n",
    "    +_find_best_split(pd.DataFrame, pd.Series) tuple\n",
    "    +_build_recursive(pd.DataFrame, pd.Series, int) Node\n",
    "    +_repr_recursive(Node, str, str) str\n",
    "    +_predict_one(pd.Series)\n",
    "    +_predict_one_recursive(Node, pd.Series)\n",
    "    +fit(pd.DataFrame, pd.Series)\n",
    "    +predict(pd.DataFrame) np.array\n",
    "}\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707d8241",
   "metadata": {},
   "source": [
    "Zorg dat onderstaande code werkt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8346fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = DecisionTree(max_depth=4)\n",
    "tree.fit(pinguins_df.drop(columns='species'), pinguins_df['species'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a2f9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree.predict(pinguins_df.drop(columns='species'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f26a49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603926d6",
   "metadata": {},
   "source": [
    "Test je decision tree ook op een andere dataset, die mag je zelf kiezen. Vergelijk de resultaten met de sklearn implemenatie van een decision tree.\n",
    "\n",
    "Zorg ook dat je laat zien dat de max_depth parameter werkt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174674ff",
   "metadata": {},
   "source": [
    "## Wat lever je in?\n",
    "- Een .py bestand met de `Node` en `DecisionTree` klasse\n",
    "- Een notebook waarin je laat zien dat je DecisionTree werkt. Let op, hierin staat niet alleen code maar ook markdown die vertelt wat je doet en wat het resultaat is.\n",
    "- De dataset die nodig is om je notebook te runnen."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-s3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
